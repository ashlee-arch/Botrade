import time
import ollama
import chromadb
import numpy as np
from sentence_transformers import SentenceTransformer
from collections import deque

# === ë©”ì‹œì§€ ê´€ë¦¬ í´ë˜ìŠ¤ ===
class Message_manager:
    def __init__(self):
        self._system_msg = {"role": "system", "content": ""}
        self.queue = deque(maxlen=10)

    def create_msg(self, role, content):
        if not isinstance(content, str):
            # ì˜ˆì™¸ì ìœ¼ë¡œ contentê°€ listë‚˜ dictì´ë©´ ë¬¸ìì—´í™”
            if isinstance(content, list):
                content = "".join(c.get("text", "") for c in content if isinstance(c, dict))
            elif isinstance(content, dict):
                import json
                content = json.dumps(content)
            else:
                content = str(content)
        return {"role": role, "content": content}

    def system_msg(self, content):
        self._system_msg = self.create_msg("system", content)

    def append_msg(self, content):
        self.queue.append(self.create_msg("user", content))

    def append_msg_by_assistant(self, content):
        self.queue.append(self.create_msg("assistant", content))

    def generate_prompt(self, retrieved_docs):
        docs = "\n".join(retrieved_docs)
        return [self._system_msg, {
            "role": "system",
            "content": f"ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:\n{docs}\nìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¤‘ì†Œê¸°ì—… ìˆ˜ì¶œ ë‹´ë‹¹ìê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. ëª¨ë¥´ëŠ” ë¶€ë¶„ì€ 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´ ì£¼ì„¸ìš”."
        }] + list(self.queue)

# === ì§ˆì˜ ì„ë² ë”© ë° ë¬¸ì„œ ê²€ìƒ‰ ===
def retrieve_docs(query, collection, embedder, top_k=2):
    start = time.time()
    query_embedding = embedder.encode(query, convert_to_tensor=False)

    if isinstance(query_embedding, np.ndarray):
        query_embedding = query_embedding.tolist()
    if isinstance(query_embedding[0], list):  # 2D â†’ 1D flatten
        query_embedding = query_embedding[0]

    print(f"ì„ë² ë”© ìƒì„±: {time.time() - start:.2f}ì´ˆ")

    start = time.time()
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)
    print(f"ë²¡í„° ê²€ìƒ‰: {time.time() - start:.2f}ì´ˆ")

    if not results["metadatas"]:
        return ["ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
    return [doc["text"] for doc in results["metadatas"][0]]

# === Ollama ì‘ë‹µ ìƒì„± ===
def generate_answer(query, retrieved_docs, model, msgManager):
    msgManager.append_msg(query)
    msg = msgManager.generate_prompt(retrieved_docs)

    print("ğŸ’¬ ë©”ì‹œì§€ êµ¬ì¡°:")
    for m in msg:
        print(m)

    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ë‹¨ê±´ ì‘ë‹µ
        response = ollama.chat(model=model, messages=msg, stream=False)
        answer = response["message"]["content"]
        print("âœ… ì‘ë‹µ:", answer)
        return answer
    except Exception as e:
        print(f"[Ollama ì˜¤ë¥˜] {e}")
        return "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
    

# === ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ===
model = "exaone3.5:latest"
embedder = SentenceTransformer("intfloat/multilingual-e5-small")
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_collection(name="rag_collection")

msgManager = Message_manager()
msgManager.system_msg(
    "ì§ˆë¬¸ìì˜ ë§íˆ¬ì™€ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì´ì–´ì§€ëŠ” ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤. "
    "í•­ìƒ ì¤‘ì†Œê¸°ì—… ìˆ˜ì¶œ ë‹´ë‹¹ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œë¥¼ ë§ë¶™ì…ë‹ˆë‹¤. "
    "ëª¨ë¥´ëŠ” ê²½ìš° 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ë©°, ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
    "ëŒ€í™”ëŠ” 'user'ì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í˜•ì‹ì´ë©°, ì´ì „ ëŒ€í™”ë„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. "
    "ê°œí–‰ì€ ë¬¸ì¥ì´ ëë‚  ë•Œì™€ ì£¼ì œê°€ ë°”ë€” ë•Œë§Œ ì‚¬ìš©í•˜ê³ , ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì€ í”¼í•©ë‹ˆë‹¤."
)

# === ë¡œì»¬ ì½˜ì†” í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ë£¨í”„ ===
def chat_loop(collection, embedder, model):
    local_msgManager = Message_manager()
    local_msgManager.system_msg(
        "ì§ˆë¬¸ìì˜ ë§íˆ¬ì™€ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì´ì–´ì§€ëŠ” ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤. "
        "í•­ìƒ ì¤‘ì†Œê¸°ì—… ìˆ˜ì¶œ ë‹´ë‹¹ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œë¥¼ ë§ë¶™ì…ë‹ˆë‹¤. "
        "ëª¨ë¥´ëŠ” ê²½ìš° 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ë©°, ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "ëŒ€í™”ëŠ” 'user'ì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í˜•ì‹ì´ë©°, ì´ì „ ëŒ€í™”ë„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. "
        "ê°œí–‰ì€ ë¬¸ì¥ì´ ëë‚  ë•Œì™€ ì£¼ì œê°€ ë°”ë€” ë•Œë§Œ ì‚¬ìš©í•˜ê³ , ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì€ í”¼í•©ë‹ˆë‹¤."
    )

    print("RAG ì±—ë´‡ ì‹œì‘! ì§ˆë¬¸ ì…ë ¥ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'):")
    while True:
        query = input("> ")
        if query.lower() == "exit":
            print("ì±—ë´‡ ì¢…ë£Œ!")
            break

        start_time = time.time()
        retrieved_docs = retrieve_docs(query, collection, embedder, top_k=2)
        answer = generate_answer(query, retrieved_docs, model, local_msgManager)
        local_msgManager.append_msg_by_assistant(answer)
        print(f"[ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ]\n")

# === ì§ì ‘ ì‹¤í–‰ ì‹œ ì½˜ì†” ëª¨ë“œ ì‹¤í–‰ ===
if __name__ == "__main__":
    import numpy as np

    model = "exaone3.5:latest"
    embedder = SentenceTransformer("intfloat/multilingual-e5-small")
    client = chromadb.PersistentClient(path="./chroma_db")
    collection = client.get_collection(name="rag_collection")

    msgManager = Message_manager()
    msgManager.system_msg(
        "ì§ˆë¬¸ìì˜ ë§íˆ¬ì™€ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, ì´ì–´ì§€ëŠ” ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤. "
        "í•­ìƒ ì¤‘ì†Œê¸°ì—… ìˆ˜ì¶œ ë‹´ë‹¹ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œë¥¼ ë§ë¶™ì…ë‹ˆë‹¤. "
        "ëª¨ë¥´ëŠ” ê²½ìš° 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ë©°, ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "ëŒ€í™”ëŠ” 'user'ì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í˜•ì‹ì´ë©°, ì´ì „ ëŒ€í™”ë„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. "
        "ê°œí–‰ì€ ë¬¸ì¥ì´ ëë‚  ë•Œì™€ ì£¼ì œê°€ ë°”ë€” ë•Œë§Œ ì‚¬ìš©í•˜ê³ , ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì€ í”¼í•©ë‹ˆë‹¤."
    )

    print("RAG ì±—ë´‡ ì‹œì‘! ì§ˆë¬¸ ì…ë ¥ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'):")
    while True:
        query = input("> ")
        if query.lower() == "exit":
            print("ì±—ë´‡ ì¢…ë£Œ!")
            break

        start = time.time()
        query_embedding = embedder.encode(query, convert_to_tensor=False)
        if isinstance(query_embedding, np.ndarray):
            query_embedding = query_embedding.tolist()
        if isinstance(query_embedding[0], list):  # 2D â†’ 1D
            query_embedding = query_embedding[0]
        print(f"ì„ë² ë”© ìƒì„±: {time.time() - start:.2f}ì´ˆ", end=' | ')

        start = time.time()
        results = collection.query(query_embeddings=[query_embedding], n_results=2)
        print(f"ë²¡í„° ê²€ìƒ‰: {time.time() - start:.2f}ì´ˆ")
        if not results["metadatas"]:
            retrieved = ["ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        else:
            retrieved = [doc["text"] for doc in results["metadatas"][0]]

        answer = generate_answer(query, retrieved, model, msgManager)
        msgManager.append_msg_by_assistant(answer)
        print(f"ë‹µë³€: {answer}\n")